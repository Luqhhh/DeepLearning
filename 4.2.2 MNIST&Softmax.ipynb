{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b466de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375be1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'if torch.cuda.is_available()else'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f6eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "if device==\"cuda\":\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81f00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.003\n",
    "batch_size=100\n",
    "training_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e0766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train=dsets.MNIST(root='Data/',\n",
    "                        train=True,\n",
    "                        transform=transforms.ToTensor(),\n",
    "                        download=True)\n",
    "mnist_test=dsets.MNIST(root='Data/',\n",
    "                       train=False,\n",
    "                       transform=transforms.ToTensor(),\n",
    "                       download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c602f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd93d40",
   "metadata": {},
   "source": [
    "Dropout 是一种常用的正则化技术，主要用于防止神经网络在训练过程中出现过拟合的问题。\n",
    "\n",
    "它的核心思想是在每个训练批次中随机“丢弃”一部分神经元（即将它们的激活值设为0），使得网络不会过分依赖某些局部特征\n",
    "\n",
    "    1.增强泛化能力： 每次训练时随机屏蔽部分神经元，让模型在不同的子网络上学习，从而减少对单个神经元的过度依赖。\n",
    "    \n",
    "    2.减少过拟合： 特别是在训练数据较少的情况下，Dropout 可以有效地抑制模型在训练集上的过拟合现象，从而在验证或测试集上获得更好的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684a73e",
   "metadata": {},
   "source": [
    "隐藏层宽度（神经元个数）\n",
    "\n",
    "参数选择的建议：\n",
    "\n",
    "1.模型容量与任务复杂性： 较大的隐藏层宽度可以让模型具有更强的表达能力，适合处理复杂任务。但如果数据量不够大或者任务本身较简单，过宽的隐藏层可能导致过拟合。\n",
    "\n",
    "2.实验调优： 建议通过交叉验证、网格搜索或其他超参数调优方法，根据任务表现来选择最合适的神经元数量。\n",
    "\n",
    "隐藏层深度（层数）：\n",
    "\n",
    "层数越多越好吗？\n",
    "\n",
    "优点： 更深的网络可以学习到更高层次、更抽象的特征，对于复杂任务来说有优势。\n",
    "\n",
    "缺点： 层数增加也会带来梯度消失或梯度爆炸的问题，使得训练过程更为困难；同时，过深的网络更容易过拟合，并且训练时间和计算资源的消耗也会大幅增加。\n",
    "\n",
    "设计建议：\n",
    "\n",
    "1.适度为宜： 选择合适的深度应结合具体任务、数据量和计算资源。实践中，深度神经网络的设计往往需要反复试验。\n",
    "\n",
    "2.辅助技术： 如果想构建更深的网络，可以考虑使用一些现代技术，如残差连接（ResNet）、Batch Normalization 等，这些方法可以缓解深层网络训练中常见的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61159398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear1=torch.nn.Linear(784,256,bias=True).to(device)\n",
    "#linear2=torch.nn.Linear(256,256,bias=True).to(device)\n",
    "#linear3=torch.nn.Linear(256,10,bias=True).to(device)\n",
    "\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "selu=torch.nn.SELU()\n",
    "dropout=torch.nn.Dropout(0.1)#p=drop_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64812c05",
   "metadata": {},
   "source": [
    "为什么在 forward 中需要激活函数？\n",
    "\n",
    "1.引入非线性：\n",
    "如果仅仅使用线性变换，无论叠加多少层，整个网络仍然等价于一个线性模型，无法拟合复杂的非线性关系。激活函数的引入使得模型可以表达任意复杂的映射关系，从而能够解决更复杂的问题。\n",
    "\n",
    "2.增强模型表达能力：\n",
    "激活函数能够让网络层与层之间建立非线性映射，这对于模式识别和函数逼近都是必不可少的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b511ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=torch.nn.Sequential(linear1,relu,linear2,relu,linear3).to(device)\n",
    "\n",
    "model = torch.nn.Sequential(linear1, selu, dropout,\n",
    "                            linear2, selu, dropout,\n",
    "                            linear3, selu, dropout,\n",
    "                            linear4, selu, dropout,\n",
    "                            linear5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb6597",
   "metadata": {},
   "source": [
    "在神经网络中，权重（weights）决定了各个输入特征如何组合并影响输出，而偏置（bias）起到平移输出的作用。\n",
    "\n",
    "在深度神经网络中，权重的初始值对训练效果起着至关重要的作用。如果初始化不当，可能会导致：\n",
    "\n",
    "   1.梯度消失或爆炸：多层网络中，梯度可能随着层数的增加不断变小或变大，从而导致网络难以训练。\n",
    "\n",
    "   2.网络收敛慢或陷入局部最优。\n",
    "\n",
    "使用 init_normal 随机初始化权重主要是为了打破对称性，使得每个神经元一开始就处于不同的状态，从而能学习到不同的特征。\n",
    "随机初始化可以帮助网络从不同角度开始优化，避免多个神经元更新方向完全一致。\n",
    "\n",
    "偏置通常初始化为0或一个很小的常数值，因为它们只负责调整激活函数的输出，不会引起神经元间的对称问题。简单的初始化（例如全0）通常已经足够。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4edbb36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0147, -0.0003, -0.0210,  ...,  0.0707, -0.0314, -0.0136],\n",
       "        [ 0.0718, -0.0103,  0.0366,  ..., -0.0319,  0.0462,  0.0303],\n",
       "        [ 0.0575, -0.0890, -0.0492,  ...,  0.0100,  0.0807, -0.0359],\n",
       "        ...,\n",
       "        [-0.0403,  0.0531, -0.0981,  ...,  0.0617, -0.0011,  0.0624],\n",
       "        [-0.0896,  0.0671,  0.0815,  ...,  0.0659,  0.1023, -0.0633],\n",
       "        [ 0.1020,  0.0481, -0.0295,  ..., -0.1030, -0.0380, -0.0858]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.nn.init.kaiming_normal_(linear1.weight,nonlinearity='relu')\n",
    "#torch.nn.init.kaiming_normal_(linear2.weight,nonlinearity='relu')\n",
    "#torch.nn.init.kaiming_normal_(linear3.weight,nonlinearity='relu')\n",
    "\n",
    "#torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "#torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "#torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c08054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e23498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Epoch:   0,Cost=0.50976586\n",
      "Epoch:   1,Cost=0.31249800\n",
      "Epoch:   2,Cost=0.27423537\n",
      "Epoch:   3,Cost=0.24632946\n",
      "Epoch:   4,Cost=0.22659017\n",
      "Epoch:   5,Cost=0.24958697\n",
      "Epoch:   6,Cost=0.21044858\n",
      "Epoch:   7,Cost=0.19861630\n",
      "Epoch:   8,Cost=0.21605150\n",
      "Epoch:   9,Cost=0.19666679\n",
      "Epoch:  10,Cost=0.24017915\n"
     ]
    }
   ],
   "source": [
    "total_batch=len(dataloader)#the num of mini_batch\n",
    "print(total_batch)\n",
    "model.train()\n",
    "for epoch in range(training_epochs+1):\n",
    "    avg_cost=0\n",
    "    for X,Y in dataloader:\n",
    "        X=X.view(-1,28*28).to(device)\n",
    "        Y=Y.to(device)\n",
    "        hypothesis=model(X)\n",
    "        cost=criterion(hypothesis,Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost+=cost/total_batch\n",
    "\n",
    "    print(\"Epoch:{:4d},Cost={:.8f}\".format(epoch,avg_cost))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca02a31",
   "metadata": {},
   "source": [
    "[r:r+1]：\n",
    "\n",
    "这是一种切片操作，它返回从索引 r 开始到 r+1（不包含 r+1）的子张量。\n",
    "返回结果的形状仍然保留了“批次维度”（即使只有一个样本），例如原始张量形状为 [N, H, W]，那么 [r:r+1] 得到的形状为 [1, H, W]。\n",
    "\n",
    "[r]：\n",
    "\n",
    "直接通过索引取出第 r 个元素，返回结果的形状会丢失批次维度。例如，从 [N, H, W] 中取出第 r 个样本后，其形状直接变为 [H, W]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24dee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:74.11000061035156%\n",
      "Prediction: 4\n",
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "    \n",
    "    X=mnist_test.data.view(-1,28*28).float().to(device)\n",
    "    Y=mnist_test.targets.to(device)\n",
    "    hypothesis=model(X)\n",
    "    cost=criterion(hypothesis,Y)\n",
    "    correct=torch.argmax(hypothesis,1)==Y\n",
    "    accuracy=correct.float().mean()*100\n",
    "    print(f\"Accuracy:{accuracy}%\")\n",
    "    \n",
    "    r=random.randint(0,len(mnist_test)-1)\n",
    "    x=mnist_test.data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    y=mnist_test.targets[r:r+1].to(device)\n",
    "    hypothesis=model(x)\n",
    "    print(\"Prediction:\",torch.argmax(hypothesis,1).item())\n",
    "    print(\"Label:\",y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421afcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oi",
   "language": "python",
   "name": "oi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
